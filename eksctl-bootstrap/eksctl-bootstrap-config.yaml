apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig
metadata:
  name: reboot-private-eks
  region: ap-south-1
  version: "1.23"
  tags:
    OwnedBy: "Saurav" 
    ManagedBy: "eksctl"

kubernetesNetworkConfig:
  ipFamily: IPv4

iam:
  withOIDC: true
  serviceAccounts:
  # - metadata:
  #     name: s3-reader
  #     # if no namespace is set, "default" will be used;
  #     # the namespace will be created if it doesn't exist already
  #     namespace: backend-apps
  #     labels: {aws-usage: "application"}
  #   attachPolicyARNs:
  #   - "arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess"
  #   tags:
  #     Owner: "John Doe"
  #     Team: "Some Team"
  # - metadata:
  #     name: cache-access
  #     namespace: backend-apps
  #     labels: {aws-usage: "application"}
  #   attachPolicyARNs:
  #   - "arn:aws:iam::aws:policy/AmazonDynamoDBReadOnlyAccess"
  #   - "arn:aws:iam::aws:policy/AmazonElastiCacheFullAccess"
  - metadata:
      name: cluster-autoscaler
      namespace: kube-system
      labels: {aws-usage: "cluster-ops"}
    wellKnownPolicies:
      autoScaler: true
    roleName: eksctl-cluster-autoscaler-role
    roleOnly: true   
          # This will only create a role for the cluster autoscaler but will not a create a service account inside k8s with ARN of role mapped in sa
  - metadata:
      name: aws-load-balancer-controller
      namespace: kube-system
      labels: {aws-usage: "cluster-ops"}
    wellKnownPolicies:
      awsLoadBalancerController: true
    roleName: eksctl-aws-loadbalancer-controller-role
    roleOnly: false         # This will only create a role for the cluster autoscaler but will not a create a service account inside k8s with ARN of role mapped in sa

iamIdentityMappings:
  - arn: arn:aws:iam::971532881571:user/reboot
    groups:
      - system:masters
    username: reboot
    noDuplicateARNs: true # prevents shadowing of ARNs

#   - arn: arn:aws:iam::000000000000:user/myUser
#     username: myUser
#     noDuplicateARNs: true

vpc:
  publicAccessCIDRs: ["14.141.28.114/32"]                  # When using this to restrict endpoint access make sure either privateAcess is true or Nodes IP is whitelisted here else they won't be able to join the cluster.
  clusterEndpoints:
    publicAccess:  true
    privateAccess: true
  nat:
    gateway: Single
  
managedNodeGroups:
  - name: ng-1-workers
    availabilityZones: ["ap-south-1b","ap-south-1c"]
    ami: ami-09115f30a400e6e15    #Add here and custom AMI ID can be found in the links section
    instanceType: t3.medium
    minSize: 1
    maxSize: 4
    desiredCapacity: 1
    volumeSize: 50
    privateNetworking: true
    propagateASGTags: true
    tags:
      nodegroup-role: worker
      k8s.io/cluster-autoscaler/enabled: "true"
      k8s.io/cluster-autoscaler/reboot-private-eks: "owned"
    iam:
      withAddonPolicies:
        externalDNS: true
        certManager: true
        autoScaler: true
        cloudWatch: true
    ssh:
      allow: true
      publicKeyPath: "./ekskey.pub"
      # enableSsm: true
    overrideBootstrapCommand: |
      #!/bin/bash
      /etc/eks/bootstrap.sh reboot-private-eks --container-runtime containerd
addons:
- name: vpc-cni
  version: v1.10.4
- name: coredns
  version: v1.8.7
- name: kube-proxy
  version: v1.23.7




  

